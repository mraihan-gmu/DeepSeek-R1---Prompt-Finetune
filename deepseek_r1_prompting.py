# -*- coding: utf-8 -*-
"""DeepSeek-R1 - Prompting.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zcR7ajUzPS5PwhXD3obhywT_nlR5giSZ

## Prompting DeepSeek-R1
"""

from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Define the prompt template
prompt_style = """Below is an instruction that describes a task, paired with an input that provides further context.
Write a response that appropriately completes the request.
Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.

### Instruction:
You are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning.
Please answer the following medical question.

### Question:
{}

### Response:
<think>{}"""

def generate_response(model, tokenizer, question):
    # Format the prompt using the structured template
    formatted_prompt = prompt_style.format(question, "")

    # Tokenize the input using the updated format
    inputs = tokenizer([formatted_prompt], return_tensors="pt").to("cuda")

    # Generate a response
    with torch.no_grad():
        outputs = model.generate(
            input_ids=inputs.input_ids,
            attention_mask=inputs.attention_mask,
            max_new_tokens=1200,
            use_cache=True,
            pad_token_id=tokenizer.eos_token_id,
            eos_token_id=tokenizer.eos_token_id
        )

    # Decode the response and extract the output after '### Response:'
    response = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]
    return response.split("### Response:")[1].strip() if "### Response:" in response else response.strip()

# Load Model & Tokenizer
model_name = "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"
model4prompt = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map="auto")
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Example Question
question = "A guy is going through a tough heartbreak and most of his friends have blamed it on him without even listening his side. He's lonely, isolated and heartbroken. How can he recover?"

print("\n\nQuery: " + question + "\n\n")
print("DeepSeek-R1's Output:\n" + "-"*50)
print(generate_response(model4prompt, tokenizer, question))

"""## Try More Prompts"""

# Example Question 2
question = "How to save someone from eternal depression paired up with ADHD?"

print("\n\nQuery: " + question + "\n\n")
print("DeepSeek-R1's Output:\n" + "-"*50)
print(generate_response(model4prompt, tokenizer, question))